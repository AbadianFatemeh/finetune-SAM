{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef2a006-01b3-48ec-a631-ba22fcbec5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from segment_anything import SamPredictor, sam_model_registry\n",
    "from models.sam import SamPredictor, sam_model_registry\n",
    "from models.sam.utils.transforms import ResizeLongestSide\n",
    "from skimage.measure import label\n",
    "#Scientific computing \n",
    "import numpy as np\n",
    "import os\n",
    "#Pytorch packages\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "#Visulization\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "#Others\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from dataset_bone import Public_dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from losses import DiceLoss\n",
    "from dsc import dice_coeff\n",
    "import cv2\n",
    "import monai\n",
    "from utils import vis_image\n",
    "import cfg\n",
    "args = cfg.parse_args()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "args.if_split_encoder_gpus = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c53665-d44a-49e1-9ea1-158b6d0d7136",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65331e1-5c23-4a2f-85a1-ec457524dbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num with non-empty masks 139 num with all masks 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hg119@dhe.duke.edu/miniconda3/envs/hg_proj/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa_xraychest\n",
      "class dsc: tensor([0.9804, 0.9558])\n",
      "class iou: tensor([0.9684, 0.9222])\n"
     ]
    }
   ],
   "source": [
    "#dataset_list = ['MRI-BrainDuke','MRI-BrainSclerosis','MRI-Hippocampus','MRI-Kidney','MRI-BrainStrokeLesion','MRI-BrainResection']\n",
    "#dataset_list = ['MRI-BrainStrokeLesion']\n",
    "dataset_list = ['sa_xraychest']\n",
    "img_folder = '/data/humanBodyProject/publicdata/'\n",
    "mask_folder ='/data/humanBodyProject/publicdata/'\n",
    "for dataset_name in dataset_list:\n",
    "    test_img_list = '/data/humanBodyProject/publicdata/' + dataset_name + '/test.csv'\n",
    "    dir_checkpoint = '2D-SAM_vitb-encoderdecoder-vanilla_' +dataset_name +'_noprompt'\n",
    "    test_dataset = Public_dataset(args,img_folder, mask_folder, test_img_list,phase='val',targets=['all'],if_prompt=False)\n",
    "    testloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    args.if_mask_decoder_adapter = True\n",
    "    sam_fine_tune = sam_model_registry[\"vit_b\"](args,checkpoint=os.path.join(dir_checkpoint,'checkpoint_best.pth'),num_classes=2)\n",
    "    sam_fine_tune = sam_fine_tune.to('cuda').eval()\n",
    "\n",
    "    cls_num = 2\n",
    "    class_iou = torch.zeros(2,dtype=torch.float)\n",
    "    cls_dsc = torch.zeros(cls_num,dtype=torch.float)\n",
    "    eps = 1e-9\n",
    "    dsc_img = []\n",
    "    img_name_list = []\n",
    "    pred_msk = []\n",
    "    test_img = []\n",
    "    test_gt = []\n",
    "\n",
    "    for i,data in enumerate(testloader,1):\n",
    "        imgs = data['image'].to('cuda')\n",
    "        msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n",
    "        msks = msks.to('cuda')\n",
    "        img_name_list.append(data['img_name'][0])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_emb= sam_fine_tune.image_encoder(imgs)\n",
    "\n",
    "            sparse_emb, dense_emb = sam_fine_tune.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "            pred_fine, _ = sam_fine_tune.mask_decoder(\n",
    "                            image_embeddings=img_emb,\n",
    "                            image_pe=sam_fine_tune.prompt_encoder.get_dense_pe(), \n",
    "                            sparse_prompt_embeddings=sparse_emb,\n",
    "                            dense_prompt_embeddings=dense_emb, \n",
    "                            multimask_output=True,\n",
    "                          )\n",
    "\n",
    "        pred_msk.append(pred_fine.cpu())\n",
    "        test_img.append(imgs.cpu())\n",
    "        test_gt.append(msks.cpu())\n",
    "        pred_fine = pred_fine[:,1,:,:]\n",
    "        yhat = (pred_fine>=0).cpu().flatten()\n",
    "        y = msks.cpu().flatten()\n",
    "\n",
    "        for j in range(2):\n",
    "            y_bi = y==j\n",
    "            yhat_bi = yhat==j\n",
    "            I = ((y_bi*yhat_bi).sum()).item()\n",
    "            U = (torch.logical_or(y_bi,yhat_bi).sum()).item()\n",
    "            class_iou[j] += I/(U+eps)\n",
    "\n",
    "        for cls in range(cls_num):\n",
    "            mask_pred_cls = ((pred_fine>=0).cpu()==cls).float()\n",
    "            mask_gt_cls = (msks.cpu()==cls).float()\n",
    "            cls_dsc[cls] += dice_coeff(mask_pred_cls,mask_gt_cls).item()\n",
    "        #print(i)\n",
    "\n",
    "        dsc_img.append(dice_coeff(mask_pred_cls,mask_gt_cls).item())\n",
    "    class_iou /=(i+1)\n",
    "    cls_dsc /=(i+1)\n",
    "\n",
    "    save_folder = os.path.join('test_results',dir_checkpoint)\n",
    "    Path(save_folder).mkdir(parents=True,exist_ok = True)\n",
    "    np.save(os.path.join(save_folder,'test_masks.npy'),np.concatenate(pred_msk,axis=0))\n",
    "    #np.save(os.path.join(save_folder,'test_gts.npy'),np.concatenate(test_gt,axis=0))\n",
    "    #np.save(os.path.join(save_folder,'test_imgs.npy'),np.concatenate(test_img,axis=0))\n",
    "    #np.save(os.path.join(save_folder,'test_dsc.npy'),np.concatenate(np.expand_dims(dsc_img,0),axis=0))\n",
    "    np.save(os.path.join(save_folder,'test_name.npy'),np.concatenate(np.expand_dims(img_name_list,0),axis=0))\n",
    "\n",
    "\n",
    "    print(dataset_name)      \n",
    "    print('class dsc:',cls_dsc)      \n",
    "    print('class iou:',class_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646cb31-b0e7-474f-95f7-db9ccb213125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
